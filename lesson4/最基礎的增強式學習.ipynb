{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "tf.reset_default_graph()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 模型搭建\n",
    "batch_size = 1\n",
    "learning_rate = 0.05\n",
    "train_w_stddev = 0.3\n",
    "# display_step = 10 #\n",
    "\n",
    "\n",
    "n_step = sample_length # h\n",
    "n_hidden = n_input\n",
    "n_classes = 1 #跟group一樣多\n",
    "\n",
    "# placeholder\n",
    "x = tf.placeholder(tf.float32, [None, n_input, n_step])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "w_in  = tf.Variable(tf.truncated_normal([n_input, n_hidden], mean=0.0, stddev=train_w_stddev, dtype=tf.float32))\n",
    "w_out = tf.Variable(tf.truncated_normal([n_hidden, n_classes], mean=0.0, stddev=train_w_stddev, dtype=tf.float32))\n",
    "b_in  = tf.Variable(tf.zeros(n_hidden, dtype = tf.float32))\n",
    "b_out = tf.Variable(tf.zeros(n_classes, dtype = tf.float32))\n",
    "\n",
    "X_in = tf.matmul(tf.reshape(x, [-1, n_input]), w_in) + b_in\n",
    "X_in = tf.reshape(X_in, [-1, n_step, n_hidden])\n",
    "\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(lstm_cell, X_in, initial_state=init_state, time_major=False)\n",
    "logits = tf.contrib.layers.fully_connected(  tf.matmul(final_state[1], w_out) + b_out , num_outputs=n_classes, activation_fn=tf.nn.sigmoid)\n",
    "p_left_and_right = tf.concat(axis=1, values=[logits,1-logits]) \n",
    "\n",
    "action = tf.multinomial(logits=tf.log(p_left_and_right), num_samples=1) \n",
    "y = 1. - tf.to_float(action) \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate) \n",
    "cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) \n",
    "grads_and_vars = optimizer.compute_gradients(cross_entropy) \n",
    "gradients = [variable for grad, variable in grads_and_vars] \n",
    "gradient_placeholders = [] \n",
    "grads_and_vars_feed = [] \n",
    "for grads, variable in grads_and_vars:\n",
    "#    X = tf.placeholder(tf.float32, shape=[None, n_inputs]) \n",
    "    gradient_placeholder = tf.placeholder(tf.float32, shape=variable.get_shape())\n",
    "    gradient_placeholders.append(gradient_placeholder)\n",
    "    grads_and_vars_feed.append((gradient_placeholder, variable)) \n",
    "training_op = optimizer.apply_gradients(grads_and_vars_feed) \n",
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 訓練程式\n",
    "n_iterations = 1000      # number of training iterations \n",
    "     # max steps per episode \n",
    "n_games_per_update = 50 # train the policy every 10 episodes \n",
    "save_iterations = 10    # save the model every 10 training iterations \n",
    "discount_rate = 1\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.6)\n",
    "samples = []\n",
    "\n",
    "all_iter_mean_rewards = [0]*int(n_iterations/save_iterations)\n",
    "with tf.Session() as sess:\n",
    "#for sss in range(0,1):\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        count = [0,0]\n",
    "        all_actions = []\n",
    "        all_rewards = []    # all sequences of raw rewards for each episode\n",
    "        all_gradients = []  # gradients saved at each step of each episode\n",
    "        for game in range(n_games_per_update):\n",
    "            current_action = [] \n",
    "            current_rewards = []   # all raw rewards from the current episode\n",
    "            current_gradients = [] # all gradients from the current episode\n",
    "            x_sample_index = np.random.randint(low=0,high=len(option_C_id_list), size=1)[0]  #取得隨機的sample index\n",
    "#            this_game_id = option_C_id_list[x_sample_index]\n",
    "#            print(this_game_id)\n",
    "#            print(x_sample_index)\n",
    "#            returns,std_x,n_max_steps = pack_sample(this_game_id,observe,avaulable_asset,variable_names,max_min_compress)\n",
    "            samples.append(std_x)\n",
    "            for step in range(0,len(returns[x_sample_index])):\n",
    "#                action_val, gradients_val = sess.run([action, gradients],feed_dict={X: obs.reshape(1, n_inputs)}) # one obs\n",
    "                logits_val,p_left_and_right_val,action_val, gradients_val = sess.run([logits,p_left_and_right,action, gradients],feed_dict={x: observes[x_sample_index][step]}) # one obs\n",
    "#                if action_val[0][0]==1:  \n",
    "#                    count_1+=1\n",
    "#                else:\n",
    "#                    count_0+=1\n",
    "                \n",
    "                this_action = action_val[0][0]\n",
    "                reward = returns[x_sample_index][step][this_action]\n",
    "#                print(reward)\n",
    "                current_action.append(this_action)\n",
    "                current_rewards.append(reward)\n",
    "                current_gradients.append(gradients_val)\n",
    "#                if done:\n",
    "#                    break\n",
    "            all_actions.append(current_action)\n",
    "            all_rewards.append(current_rewards)\n",
    "            all_gradients.append(current_gradients)\n",
    "        # At this point we have run the policy for 10 episodes, and we are\n",
    "        # ready for a policy update using the algorithm described earlier.\n",
    "#        all_rewards = discount_and_normalize_rewards(all_rewards,discount_rate)\n",
    "        feed_dict = {}\n",
    "        for var_index, grad_placeholder in enumerate(gradient_placeholders):\n",
    "            # multiply the gradients by the action scores, and compute the mean\n",
    "            mean_gradients = np.mean([reward * all_gradients[game_index][step][var_index]for game_index, rewards in enumerate(all_rewards)for step, reward in enumerate(rewards)],axis=0)\n",
    "            feed_dict[grad_placeholder] = mean_gradients\n",
    "        sess.run(training_op, feed_dict=feed_dict)\n",
    "        if iteration % save_iterations == 0:\n",
    "            count_predict = [0,0]\n",
    "            test1 = 0\n",
    "            test = 0\n",
    "            saver.save(sess, \"./back_test_option_VOL/my_policy_net_pg1.ckpt\")                 \n",
    "            all_actions_predict = []\n",
    "            all_rewards_predict = []    # all sequences of raw rewards for each episode\n",
    "            all_gradients_predict = []  # gradients saved at each step of each episode\n",
    "            for game in range(0,len(option_C_id_list)):\n",
    "                current_action_predict = [] \n",
    "                current_rewards_predict = []   # all raw rewards from the current episode\n",
    "                current_gradients_predict = [] # all gradients from the current episode\n",
    "                x_sample_index = game\n",
    "                test1+=1\n",
    "#                returns,std_x,n_max_steps = pack_sample(this_game_id,observe,avaulable_asset,variable_names,max_min_compress)\n",
    "                samples.append(std_x)\n",
    "                for step in range(0,len(returns[x_sample_index])):\n",
    "    #                action_val, gradients_val = sess.run([action, gradients],feed_dict={X: obs.reshape(1, n_inputs)}) # one obs\n",
    "                    action_val = sess.run(action,feed_dict={x: observes[x_sample_index][step]}) # one obs\n",
    "                    this_action = action_val[0][0]\n",
    "                    if this_action==1:  \n",
    "                        count_predict[1]+=1\n",
    "                    else:\n",
    "                        count_predict[0]+=1\n",
    "                    \n",
    "                    test +=1\n",
    "                    reward_predict = returns[x_sample_index][step][this_action] #*(max_min_compress['roi']['max'] -max_min_compress['roi']['min']) + max_min_compress['roi']['min']\n",
    "\n",
    "                    current_action_predict.append(this_action)\n",
    "                    current_rewards_predict.append(reward_predict)\n",
    "                all_actions_predict.append(current_action_predict)\n",
    "                all_rewards_predict.append(np.mean(current_rewards_predict))\n",
    "            mean_reward = np.mean(all_rewards_predict)\n",
    "#            all_iter_mean_rewards[int(iteration/save_iterations)] = np.mean(all_rewards)\n",
    "            print(test)\n",
    "            print(str(iteration/n_iterations)+\" return:\"+str(mean_reward)+\" 1 count:\"+str(count_predict[1])+\" / 0 count:\"+str(count_predict[0]))            \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
